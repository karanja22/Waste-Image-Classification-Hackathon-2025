{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2ggrntxt2gj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "import shutil  # Added for deletion\n",

        "# Define Paths\n",
        "zip_path = \"/content/Waste Classification Dataset.zip\"\n",
        "extract_path = \"/content/Waste_Classification\"\n",
        "\n",
        "saved_images_dir = \"/content/Processed Data/preprocessed_batches\"\n",
        "saved_labels_path = \"/content/Processed Data/preprocessed_labels.npy\"\n",
        "\n",
        "# Force Re-Extraction: Delete old extracted folder and re-extract dataset\n",
        "if os.path.exists(extract_path):\n",
        "    print(\"Deleting old extracted dataset...\")\n",
        "    shutil.rmtree(extract_path)  # Delete the extracted folder\n",
        "\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "    z.extractall(extract_path)\n",
        "print(\"Extraction complete!\")\n",
        "\n",
        "# Re-check total images **AFTER** extraction\n",
        "total_images = sum([len(files) for _, _, files in os.walk(extract_path) if any(file.endswith((\".jpg\")) for file in files)])\n",
        "print(f\"Total extracted images: {total_images}\")\n",
        "\n",
        "# Load Image Paths & Labels\n",
        "labels, img_paths = [], []\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    category = os.path.basename(root)\n",
        "    if category in [\"recyclable\", \"organic\"]:  # Adjust categories if needed\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".jpg\")):\n",
        "                labels.append(category)\n",
        "                img_paths.append(os.path.join(root, file))\n",
        "\n",
        "# Debugging: Check Total Images & Labels\n",
        "print(f\"Total images found: {len(img_paths)}\")\n",
        "print(f\"Total labels found: {len(labels)}\")\n",
        "assert len(img_paths) == len(labels), \"Mismatch between images and labels!\"\n",
        "\n",
        "#Encode Labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "#Ensure Directory Exists Before Saving Labels\n",
        "os.makedirs(os.path.dirname(saved_labels_path), exist_ok=True)\n",
        "\n",
        "#Save Labels to Drive\n",
        "np.save(saved_labels_path, y)\n",
        "print(f\"Saved {len(y)} labels successfully.\")\n",
        "\n",
        "#DELETE OLD PREPROCESSED BATCHES BEFORE PROCESSING NEW ONES\n",
        "if os.path.exists(saved_images_dir):\n",
        "    print(\"Deleting old preprocessed image batches...\")\n",
        "    shutil.rmtree(saved_images_dir)\n",
        "\n",
        "#Standardization Function\n",
        "def standardize_images(images):\n",
        "    \"\"\"Standardize images: subtract mean and divide by standard deviation.\"\"\"\n",
        "    mean = np.mean(images, axis=(0, 1, 2), keepdims=True)\n",
        "    std = np.std(images, axis=(0, 1, 2), keepdims=True)\n",
        "    return (images - mean) / (std + 1e-7)  # Avoid division by zero\n",
        "\n",
        "#Batch Processing Function\n",
        "def preprocess_images(image_paths, batch_size=500):\n",
        "    \"\"\"Process images in batches and save them separately to prevent memory overload.\"\"\"\n",
        "    os.makedirs(saved_images_dir, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "    for i in range(0, len(image_paths), batch_size):\n",
        "        batch_file_path = os.path.join(saved_images_dir, f\"batch_{i // batch_size}.npy\")\n",
        "\n",
        "        images = []\n",
        "        for img_path in image_paths[i : i + batch_size]:  # Process batch\n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"RGB\").resize((256, 256))\n",
        "                img = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
        "                images.append(img)\n",
        "            except Exception as e:\n",
        "                print(f\"âš  Skipping corrupted image: {img_path}\")\n",
        "\n",
        "        batch = np.array(images)\n",
        "        np.save(batch_file_path, batch)  # Save batch\n",
        "        print(f\"Saved batch {i // batch_size}: {batch.shape}\")\n",
        "\n",
        "#Process & Save Images in Batches\n",
        "print(\"Preprocessing images in batches...\")\n",
        "preprocess_images(img_paths, batch_size=500)\n",
        "print(\"Image preprocessing complete!\")\n"
      ]
    }
  ]
}
