{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import zipfile\n",
    "# import shutil\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from PIL import Image\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from collections import Counter\n",
    "# import random\n",
    "\n",
    "# # âœ… Define Paths\n",
    "# ZIP_PATH = \"Waste Classification Dataset.zip\"\n",
    "# EXTRACT_PATH = \"Waste_Classification\"\n",
    "# SAVED_IMAGES_PATH = \"Processed_Data/images.npy\"\n",
    "# SAVED_LABELS_PATH = \"Processed_Data/labels.npy\"\n",
    "\n",
    "# # âœ… Force Re-Extraction: Delete old dataset and re-extract\n",
    "# if os.path.exists(EXTRACT_PATH):\n",
    "#     print(\"ğŸš¨ Deleting old extracted dataset...\")\n",
    "#     shutil.rmtree(EXTRACT_PATH)\n",
    "\n",
    "# print(\"ğŸ“‚ Extracting dataset...\")\n",
    "# with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "#     z.extractall(EXTRACT_PATH)\n",
    "# print(\"âœ… Extraction complete!\")\n",
    "\n",
    "# # âœ… Load Image Paths & Labels\n",
    "# labels, img_paths = [], []\n",
    "# for root, dirs, files in os.walk(EXTRACT_PATH):\n",
    "#     category = os.path.basename(root)\n",
    "#     if category in [\"recyclable\", \"organic\"]:  # Adjust categories if needed\n",
    "#         for file in files:\n",
    "#             if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#                 labels.append(category)\n",
    "#                 img_paths.append(os.path.join(root, file))\n",
    "\n",
    "# print(f\"ğŸ”¹ Total images found: {len(img_paths)}\")\n",
    "# print(f\"ğŸ”¹ Total labels found: {len(labels)}\")\n",
    "# assert len(img_paths) == len(labels), \"âŒ Mismatch between images and labels!\"\n",
    "\n",
    "# # âœ… Balance the Dataset (Undersampling to Match the Smallest Class)\n",
    "# class_counts = Counter(labels)\n",
    "# min_count = min(class_counts.values())  # Get the smallest class size\n",
    "# print(f\"ğŸ” Class distribution before balancing: {class_counts}\")\n",
    "\n",
    "# balanced_img_paths, balanced_labels = [], []\n",
    "# for category in class_counts.keys():\n",
    "#     category_indices = [i for i, lbl in enumerate(labels) if lbl == category]\n",
    "#     sampled_indices = random.sample(category_indices, min_count)  # Undersampling\n",
    "    \n",
    "#     for idx in sampled_indices:\n",
    "#         balanced_img_paths.append(img_paths[idx])\n",
    "#         balanced_labels.append(labels[idx])\n",
    "\n",
    "# print(f\"âœ… Class distribution after balancing: {Counter(balanced_labels)}\")\n",
    "\n",
    "# # âœ… Replace original lists with balanced versions\n",
    "# img_paths = balanced_img_paths\n",
    "# labels = balanced_labels\n",
    "\n",
    "# # âœ… Encode Labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# # âœ… Ensure Directory Exists Before Saving Labels & Images\n",
    "# os.makedirs(os.path.dirname(SAVED_LABELS_PATH), exist_ok=True)\n",
    "\n",
    "# # âœ… Save Labels\n",
    "# np.save(SAVED_LABELS_PATH, y)\n",
    "# print(f\"ğŸ“ Saved {len(y)} labels successfully.\")\n",
    "\n",
    "# # âœ… Image Processing Function (Optimized)\n",
    "# IMG_SIZE = (128, 128)\n",
    "\n",
    "# def preprocess_image(img_path):\n",
    "#     \"\"\"Loads and preprocesses an image (resizing, normalization).\"\"\"\n",
    "#     try:\n",
    "#         img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "#         img = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
    "#         return img\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš  Skipping corrupted image: {img_path}\")\n",
    "#         return None\n",
    "\n",
    "# # âœ… Use ThreadPoolExecutor for Faster Processing\n",
    "# print(\"ğŸ”„ Processing images using multiprocessing...\")\n",
    "# with ThreadPoolExecutor(max_workers=8) as executor:  # Use 8 threads for speed\n",
    "#     images = list(executor.map(preprocess_image, img_paths))\n",
    "\n",
    "# # âœ… Remove failed loads (None values)\n",
    "# valid_data = [(img, label) for img, label in zip(images, y) if img is not None]\n",
    "\n",
    "# # âœ… Split into separate arrays\n",
    "# X, y = zip(*valid_data)  \n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # âœ… Save Processed Images\n",
    "# np.save(SAVED_IMAGES_PATH, X)\n",
    "# print(f\"âœ… Saved processed images! Shape: {X.shape}\")\n",
    "\n",
    "# # âœ… Load Dataset from Saved Files\n",
    "# print(\"ğŸ“‚ Loading saved dataset...\")\n",
    "# X = np.load(SAVED_IMAGES_PATH, mmap_mode=\"r\")\n",
    "# y = np.load(SAVED_LABELS_PATH, mmap_mode=\"r\")\n",
    "# print(f\"ğŸ“Š Loaded images: {X.shape}, Labels: {y.shape}\")\n",
    "\n",
    "# # âœ… Create TensorFlow Dataset (Efficient Streaming)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "# # âœ… Apply Data Augmentation for Robustness\n",
    "# def augment(image, label):\n",
    "#     image = tf.image.random_flip_left_right(image)\n",
    "#     image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "#     return image, label\n",
    "\n",
    "# dataset = dataset.shuffle(len(X)).map(augment).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "# print(\"ğŸš€ Dataset ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Deleting old extracted dataset...\n",
      "ğŸ“‚ Extracting dataset...\n",
      "âœ… Extraction complete!\n",
      "ğŸ”¹ Total images found: 24705\n",
      "ğŸ”¹ Total labels found: 24705\n",
      "ğŸ” Class distribution before balancing: Counter({'organic': 13880, 'recyclable': 10825})\n",
      "âœ… Class distribution after balancing: Counter({'organic': 10825, 'recyclable': 10825})\n",
      "ğŸ“ Saved 21650 labels successfully.\n",
      "ğŸ”„ Processing images using multiprocessing...\n",
      "âœ… Saved processed images! Shape: (21650, 128, 128, 3)\n",
      "ğŸ“‚ Loading saved dataset...\n",
      "ğŸ“Š Loaded images: (21650, 128, 128, 3), Labels: (21650,)\n",
      "ğŸš€ Dataset ready for training!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# âœ… Define Paths\n",
    "ZIP_PATH = \"Waste Classification Dataset.zip\"\n",
    "EXTRACT_PATH = \"Waste_Classification\"\n",
    "SAVED_IMAGES_PATH = \"Processed_Data/images.npy\"\n",
    "SAVED_LABELS_PATH = \"Processed_Data/labels.npy\"\n",
    "\n",
    "# âœ… Force Re-Extraction: Delete old dataset and re-extract\n",
    "if os.path.exists(EXTRACT_PATH):\n",
    "    print(\"ğŸš¨ Deleting old extracted dataset...\")\n",
    "    shutil.rmtree(EXTRACT_PATH)\n",
    "\n",
    "print(\"ğŸ“‚ Extracting dataset...\")\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "    z.extractall(EXTRACT_PATH)\n",
    "print(\"âœ… Extraction complete!\")\n",
    "\n",
    "# âœ… Load Image Paths & Labels\n",
    "labels, img_paths = [], []\n",
    "for root, dirs, files in os.walk(EXTRACT_PATH):\n",
    "    category = os.path.basename(root)\n",
    "    if category in [\"recyclable\", \"organic\"]:\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                labels.append(category)\n",
    "                img_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ğŸ”¹ Total images found: {len(img_paths)}\")\n",
    "print(f\"ğŸ”¹ Total labels found: {len(labels)}\")\n",
    "assert len(img_paths) == len(labels), \"âŒ Mismatch between images and labels!\"\n",
    "\n",
    "# âœ… Balance the Dataset (Undersampling to Match the Smallest Class)\n",
    "class_counts = Counter(labels)\n",
    "min_count = min(class_counts.values())\n",
    "print(f\"ğŸ” Class distribution before balancing: {class_counts}\")\n",
    "\n",
    "balanced_img_paths, balanced_labels = [], []\n",
    "for category in class_counts.keys():\n",
    "    category_indices = [i for i, lbl in enumerate(labels) if lbl == category]\n",
    "    sampled_indices = random.sample(category_indices, min_count)\n",
    "    \n",
    "    for idx in sampled_indices:\n",
    "        balanced_img_paths.append(img_paths[idx])\n",
    "        balanced_labels.append(labels[idx])\n",
    "\n",
    "print(f\"âœ… Class distribution after balancing: {Counter(balanced_labels)}\")\n",
    "\n",
    "# âœ… Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(balanced_labels)\n",
    "\n",
    "# âœ… Ensure Directory Exists Before Saving Labels & Images\n",
    "os.makedirs(os.path.dirname(SAVED_LABELS_PATH), exist_ok=True)\n",
    "\n",
    "# âœ… Save Labels\n",
    "np.save(SAVED_LABELS_PATH, y)\n",
    "print(f\"ğŸ“ Saved {len(y)} labels successfully.\")\n",
    "\n",
    "# âœ… Image Processing Function\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Skipping corrupted image: {img_path}\")\n",
    "        return None\n",
    "\n",
    "# âœ… Use ThreadPoolExecutor for Faster Processing\n",
    "print(\"ğŸ”„ Processing images using multiprocessing...\")\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    images = list(executor.map(preprocess_image, balanced_img_paths))\n",
    "\n",
    "# âœ… Remove failed loads\n",
    "valid_data = [(img, label) for img, label in zip(images, y) if img is not None]\n",
    "\n",
    "# âœ… Convert to NumPy Arrays\n",
    "X, y = zip(*valid_data)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# âœ… Save Processed Images\n",
    "np.save(SAVED_IMAGES_PATH, X)\n",
    "print(f\"âœ… Saved processed images! Shape: {X.shape}\")\n",
    "\n",
    "# âœ… Load Dataset from Saved Files\n",
    "print(\"ğŸ“‚ Loading saved dataset...\")\n",
    "X = np.load(SAVED_IMAGES_PATH)\n",
    "y = np.load(SAVED_LABELS_PATH)\n",
    "print(f\"ğŸ“Š Loaded images: {X.shape}, Labels: {y.shape}\")\n",
    "\n",
    "# # âœ… Ensure TensorFlow Dataset Works with Tensors\n",
    "# def augment(image, label):\n",
    "#     image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "#     image = tf.image.random_flip_left_right(image)\n",
    "#     image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "#     return image, label\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# dataset = dataset.shuffle(len(X)).map(augment).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"ğŸš€ Dataset ready for training!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
