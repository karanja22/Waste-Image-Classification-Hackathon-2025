{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import zipfile\n",
    "# import shutil\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from PIL import Image\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from collections import Counter\n",
    "# import random\n",
    "\n",
    "# # ‚úÖ Define Paths\n",
    "# ZIP_PATH = \"Waste Classification Dataset.zip\"\n",
    "# EXTRACT_PATH = \"Waste_Classification\"\n",
    "# SAVED_IMAGES_PATH = \"Processed_Data/images.npy\"\n",
    "# SAVED_LABELS_PATH = \"Processed_Data/labels.npy\"\n",
    "\n",
    "# # ‚úÖ Force Re-Extraction: Delete old dataset and re-extract\n",
    "# if os.path.exists(EXTRACT_PATH):\n",
    "#     print(\"üö® Deleting old extracted dataset...\")\n",
    "#     shutil.rmtree(EXTRACT_PATH)\n",
    "\n",
    "# print(\"üìÇ Extracting dataset...\")\n",
    "# with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "#     z.extractall(EXTRACT_PATH)\n",
    "# print(\"‚úÖ Extraction complete!\")\n",
    "\n",
    "# # ‚úÖ Load Image Paths & Labels\n",
    "# labels, img_paths = [], []\n",
    "# for root, dirs, files in os.walk(EXTRACT_PATH):\n",
    "#     category = os.path.basename(root)\n",
    "#     if category in [\"recyclable\", \"organic\"]:  # Adjust categories if needed\n",
    "#         for file in files:\n",
    "#             if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#                 labels.append(category)\n",
    "#                 img_paths.append(os.path.join(root, file))\n",
    "\n",
    "# print(f\"üîπ Total images found: {len(img_paths)}\")\n",
    "# print(f\"üîπ Total labels found: {len(labels)}\")\n",
    "# assert len(img_paths) == len(labels), \"‚ùå Mismatch between images and labels!\"\n",
    "\n",
    "# # ‚úÖ Balance the Dataset (Undersampling to Match the Smallest Class)\n",
    "# class_counts = Counter(labels)\n",
    "# min_count = min(class_counts.values())  # Get the smallest class size\n",
    "# print(f\"üîç Class distribution before balancing: {class_counts}\")\n",
    "\n",
    "# balanced_img_paths, balanced_labels = [], []\n",
    "# for category in class_counts.keys():\n",
    "#     category_indices = [i for i, lbl in enumerate(labels) if lbl == category]\n",
    "#     sampled_indices = random.sample(category_indices, min_count)  # Undersampling\n",
    "    \n",
    "#     for idx in sampled_indices:\n",
    "#         balanced_img_paths.append(img_paths[idx])\n",
    "#         balanced_labels.append(labels[idx])\n",
    "\n",
    "# print(f\"‚úÖ Class distribution after balancing: {Counter(balanced_labels)}\")\n",
    "\n",
    "# # ‚úÖ Replace original lists with balanced versions\n",
    "# img_paths = balanced_img_paths\n",
    "# labels = balanced_labels\n",
    "\n",
    "# # ‚úÖ Encode Labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# # ‚úÖ Ensure Directory Exists Before Saving Labels & Images\n",
    "# os.makedirs(os.path.dirname(SAVED_LABELS_PATH), exist_ok=True)\n",
    "\n",
    "# # ‚úÖ Save Labels\n",
    "# np.save(SAVED_LABELS_PATH, y)\n",
    "# print(f\"üìÅ Saved {len(y)} labels successfully.\")\n",
    "\n",
    "# # ‚úÖ Image Processing Function (Optimized)\n",
    "# IMG_SIZE = (128, 128)\n",
    "\n",
    "# def preprocess_image(img_path):\n",
    "#     \"\"\"Loads and preprocesses an image (resizing, normalization).\"\"\"\n",
    "#     try:\n",
    "#         img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "#         img = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
    "#         return img\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö† Skipping corrupted image: {img_path}\")\n",
    "#         return None\n",
    "\n",
    "# # ‚úÖ Use ThreadPoolExecutor for Faster Processing\n",
    "# print(\"üîÑ Processing images using multiprocessing...\")\n",
    "# with ThreadPoolExecutor(max_workers=8) as executor:  # Use 8 threads for speed\n",
    "#     images = list(executor.map(preprocess_image, img_paths))\n",
    "\n",
    "# # ‚úÖ Remove failed loads (None values)\n",
    "# valid_data = [(img, label) for img, label in zip(images, y) if img is not None]\n",
    "\n",
    "# # ‚úÖ Split into separate arrays\n",
    "# X, y = zip(*valid_data)  \n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # ‚úÖ Save Processed Images\n",
    "# np.save(SAVED_IMAGES_PATH, X)\n",
    "# print(f\"‚úÖ Saved processed images! Shape: {X.shape}\")\n",
    "\n",
    "# # ‚úÖ Load Dataset from Saved Files\n",
    "# print(\"üìÇ Loading saved dataset...\")\n",
    "# X = np.load(SAVED_IMAGES_PATH, mmap_mode=\"r\")\n",
    "# y = np.load(SAVED_LABELS_PATH, mmap_mode=\"r\")\n",
    "# print(f\"üìä Loaded images: {X.shape}, Labels: {y.shape}\")\n",
    "\n",
    "# # ‚úÖ Create TensorFlow Dataset (Efficient Streaming)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "# # ‚úÖ Apply Data Augmentation for Robustness\n",
    "# def augment(image, label):\n",
    "#     image = tf.image.random_flip_left_right(image)\n",
    "#     image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "#     return image, label\n",
    "\n",
    "# dataset = dataset.shuffle(len(X)).map(augment).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "# print(\"üöÄ Dataset ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Deleting old extracted dataset...\n",
      "üìÇ Extracting dataset...\n",
      "‚úÖ Extraction complete!\n",
      "üîπ Total images found: 24705\n",
      "üîπ Total labels found: 24705\n",
      "üîç Class distribution before balancing: Counter({'organic': 13880, 'recyclable': 10825})\n",
      "‚úÖ Class distribution after balancing: Counter({'organic': 10825, 'recyclable': 10825})\n",
      "üìÅ Saved 21650 labels successfully.\n",
      "üîÑ Processing images using multiprocessing...\n",
      "‚úÖ Saved processed images! Shape: (21650, 128, 128, 3)\n",
      "üìÇ Loading saved dataset...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.96 GiB for an array with shape (1064140800,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# ‚úÖ Load Dataset from Saved Files\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÇ Loading saved dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVED_IMAGES_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m y = np.load(SAVED_LABELS_PATH)\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Loaded images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Engineer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    453\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.open_memmap(file, mode=mmap_mode,\n\u001b[32m    454\u001b[39m                                   max_header_size=max_header_size)\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Engineer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\format.py:809\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[32m    808\u001b[39m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m         array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    811\u001b[39m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[32m    812\u001b[39m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[32m    821\u001b[39m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[32m    822\u001b[39m         array = numpy.ndarray(count, dtype=dtype)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.96 GiB for an array with shape (1064140800,) and data type float32"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# ‚úÖ Define Paths\n",
    "ZIP_PATH = \"Waste Classification Dataset.zip\"\n",
    "EXTRACT_PATH = \"Waste_Classification\"\n",
    "SAVED_IMAGES_PATH = \"Processed_Data/images.npy\"\n",
    "SAVED_LABELS_PATH = \"Processed_Data/labels.npy\"\n",
    "\n",
    "# ‚úÖ Force Re-Extraction: Delete old dataset and re-extract\n",
    "if os.path.exists(EXTRACT_PATH):\n",
    "    print(\"üö® Deleting old extracted dataset...\")\n",
    "    shutil.rmtree(EXTRACT_PATH)\n",
    "\n",
    "print(\"üìÇ Extracting dataset...\")\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "    z.extractall(EXTRACT_PATH)\n",
    "print(\"‚úÖ Extraction complete!\")\n",
    "\n",
    "# ‚úÖ Load Image Paths & Labels\n",
    "labels, img_paths = [], []\n",
    "for root, dirs, files in os.walk(EXTRACT_PATH):\n",
    "    category = os.path.basename(root)\n",
    "    if category in [\"recyclable\", \"organic\"]:\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                labels.append(category)\n",
    "                img_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"üîπ Total images found: {len(img_paths)}\")\n",
    "print(f\"üîπ Total labels found: {len(labels)}\")\n",
    "assert len(img_paths) == len(labels), \"‚ùå Mismatch between images and labels!\"\n",
    "\n",
    "# ‚úÖ Balance the Dataset (Undersampling to Match the Smallest Class)\n",
    "class_counts = Counter(labels)\n",
    "min_count = min(class_counts.values())\n",
    "print(f\"üîç Class distribution before balancing: {class_counts}\")\n",
    "\n",
    "balanced_img_paths, balanced_labels = [], []\n",
    "for category in class_counts.keys():\n",
    "    category_indices = [i for i, lbl in enumerate(labels) if lbl == category]\n",
    "    sampled_indices = random.sample(category_indices, min_count)\n",
    "    \n",
    "    for idx in sampled_indices:\n",
    "        balanced_img_paths.append(img_paths[idx])\n",
    "        balanced_labels.append(labels[idx])\n",
    "\n",
    "print(f\"‚úÖ Class distribution after balancing: {Counter(balanced_labels)}\")\n",
    "\n",
    "# ‚úÖ Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(balanced_labels)\n",
    "\n",
    "# ‚úÖ Ensure Directory Exists Before Saving Labels & Images\n",
    "os.makedirs(os.path.dirname(SAVED_LABELS_PATH), exist_ok=True)\n",
    "\n",
    "# ‚úÖ Save Labels\n",
    "np.save(SAVED_LABELS_PATH, y)\n",
    "print(f\"üìÅ Saved {len(y)} labels successfully.\")\n",
    "\n",
    "# ‚úÖ Image Processing Function\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Skipping corrupted image: {img_path}\")\n",
    "        return None\n",
    "\n",
    "# ‚úÖ Use ThreadPoolExecutor for Faster Processing\n",
    "print(\"üîÑ Processing images using multiprocessing...\")\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    images = list(executor.map(preprocess_image, balanced_img_paths))\n",
    "\n",
    "# ‚úÖ Remove failed loads\n",
    "valid_data = [(img, label) for img, label in zip(images, y) if img is not None]\n",
    "\n",
    "# ‚úÖ Convert to NumPy Arrays\n",
    "X, y = zip(*valid_data)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# ‚úÖ Save Processed Images\n",
    "np.save(SAVED_IMAGES_PATH, X)\n",
    "print(f\"‚úÖ Saved processed images! Shape: {X.shape}\")\n",
    "\n",
    "# ‚úÖ Load Dataset from Saved Files\n",
    "print(\"üìÇ Loading saved dataset...\")\n",
    "X = np.load(SAVED_IMAGES_PATH)\n",
    "y = np.load(SAVED_LABELS_PATH)\n",
    "print(f\"üìä Loaded images: {X.shape}, Labels: {y.shape}\")\n",
    "\n",
    "# ‚úÖ Ensure TensorFlow Dataset Works with Tensors\n",
    "def augment(image, label):\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    return image, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(len(X)).map(augment).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"üöÄ Dataset ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
